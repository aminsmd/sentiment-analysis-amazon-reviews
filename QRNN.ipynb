{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 50000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "NB_START_EPOCHS = 26  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data) :\n",
    "    labels = []\n",
    "    for i in data :\n",
    "        if(((i.split()[0]).replace(\"__label__\",\"\"))=='1'):\n",
    "            labels.append([1,0])\n",
    "        else :\n",
    "            labels.append([0,1])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[0].replace(\"__label__\",\"\"))[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'!'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_label(data):\n",
    "    d = []\n",
    "    for i in data :\n",
    "        d.append((i.replace(\"__label__\",\"\"))[2:])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_label(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:20000]\n",
    "labels = labels[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_sentences(data):    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = remove_stopwords(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_sentences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \", \n",
    "               char_level=False)\n",
    "tk.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "training = []\n",
    "for i in range(len(data)):\n",
    "    training.append(tokenizer.tokenize(data[i]))\n",
    "for i in range(len(training)):\n",
    "    training[i] = [x.lower() for x in training[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.word_index['<PAD>']=NB_WORDS+1\n",
    "tk.word_index['<UNK>']=0\n",
    "tk.word_index['n\\'t'] = NB_WORDS+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "train_X =[]\n",
    "for i in range(len(training)):\n",
    "    sentence = []\n",
    "    for j in range(len(training[i])):\n",
    "        if training[i][j] in punct:\n",
    "            pass\n",
    "        else:\n",
    "            sentence.append(training[i][j])\n",
    "    train_X.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 120\n",
    "for s in range(len(train_X)):\n",
    "    n = MAX_SEQ - len(train_X[s])\n",
    "    if n < 0:\n",
    "        train_X[s] = train_X[s][:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            train_X[s].append('<PAD>')\n",
    "    for v in range(len(train_X[s])):\n",
    "        if train_X[s][v] not in tk.word_index:\n",
    "            train_X[s][v] = tk.word_index['<UNK>']\n",
    "        else:\n",
    "            train_X[s][v] = tk.word_index[train_X[s][v]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tk.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 120)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.array(train_X)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 120, 32)           1600096   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               106400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 1,706,898\n",
      "Trainable params: 1,706,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(NB_WORDS+3, embedding_size, input_length=MAX_SEQ))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.8)))\n",
    "# model.add(Dense(20, kernel_regularizer=regularizers.l1_l2(0.01)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='adam'\n",
    "              , loss='binary_crossentropy'\n",
    "              , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.layers import Layer, InputSpec\n",
    "\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "def _dropout(x, level, noise_shape=None, seed=None):\n",
    "    x = K.dropout(x, level, noise_shape, seed)\n",
    "    x *= (1. - level) # compensate for the scaling by the dropout\n",
    "    return x\n",
    "\n",
    "\n",
    "class QRNN(Layer):\n",
    "    '''Quasi RNN\n",
    "    # Arguments\n",
    "        units: dimension of the internal projections and the final output.\n",
    "    # References\n",
    "        - [Quasi-recurrent Neural Networks](http://arxiv.org/abs/1611.01576)\n",
    "    '''\n",
    "    def __init__(self, units, window_size=2, stride=1,\n",
    "                 return_sequences=False, go_backwards=False, \n",
    "                 stateful=False, unroll=False, activation='tanh',\n",
    "                 kernel_initializer='uniform', bias_initializer='zero',\n",
    "                 kernel_regularizer=None, bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None, bias_constraint=None, \n",
    "                 dropout=0, use_bias=True, input_dim=None, input_length=None,\n",
    "                 **kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        self.go_backwards = go_backwards\n",
    "        self.stateful = stateful\n",
    "        self.unroll = unroll\n",
    "\n",
    "        self.units = units \n",
    "        self.window_size = window_size\n",
    "        self.strides = (stride, 1)\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.supports_masking = True\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        self.input_dim = input_dim\n",
    "        self.input_length = input_length\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_length, self.input_dim)\n",
    "        super(QRNN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        batch_size = input_shape[0] if self.stateful else None\n",
    "        self.input_dim = input_shape[2]\n",
    "        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n",
    "        self.state_spec = InputSpec(shape=(batch_size, self.units))\n",
    "\n",
    "        self.states = [None]\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "\n",
    "        kernel_shape = (self.window_size, 1, self.input_dim, self.units * 3)\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name='bias', \n",
    "                                        shape=(self.units * 3,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        length = input_shape[1]\n",
    "        if length:\n",
    "            length = conv_output_length(length + self.window_size - 1,\n",
    "                                        self.window_size, 'valid',\n",
    "                                        self.strides[0])\n",
    "        if self.return_sequences:\n",
    "            return (input_shape[0], length, self.units)\n",
    "        else:\n",
    "            return (input_shape[0], self.units)\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        if self.return_sequences:\n",
    "            return mask\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_initial_states(self, inputs):\n",
    "        # build an all-zero tensor of shape (samples, units)\n",
    "        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n",
    "        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n",
    "        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n",
    "        initial_state = K.tile(initial_state, [1, self.units])  # (samples, units)\n",
    "        initial_states = [initial_state for _ in range(len(self.states))]\n",
    "        return initial_states\n",
    "\n",
    "    def reset_states(self, states=None):\n",
    "        if not self.stateful:\n",
    "            raise AttributeError('Layer must be stateful.')\n",
    "        if not self.input_spec:\n",
    "            raise RuntimeError('Layer has never been called '\n",
    "                               'and thus has no states.')\n",
    "\n",
    "        batch_size = self.input_spec.shape[0]\n",
    "        if not batch_size:\n",
    "            raise ValueError('If a QRNN is stateful, it needs to know '\n",
    "                             'its batch size. Specify the batch size '\n",
    "                             'of your input tensors: \\n'\n",
    "                             '- If using a Sequential model, '\n",
    "                             'specify the batch size by passing '\n",
    "                             'a `batch_input_shape` '\n",
    "                             'argument to your first layer.\\n'\n",
    "                             '- If using the functional API, specify '\n",
    "                             'the time dimension by passing a '\n",
    "                             '`batch_shape` argument to your Input layer.')\n",
    "\n",
    "        if self.states[0] is None:\n",
    "            self.states = [K.zeros((batch_size, self.units))\n",
    "                           for _ in self.states]\n",
    "        elif states is None:\n",
    "            for state in self.states:\n",
    "                K.set_value(state, np.zeros((batch_size, self.units)))\n",
    "        else:\n",
    "            if not isinstance(states, (list, tuple)):\n",
    "                states = [states]\n",
    "            if len(states) != len(self.states):\n",
    "                raise ValueError('Layer ' + self.name + ' expects ' +\n",
    "                                 str(len(self.states)) + ' states, '\n",
    "                                 'but it received ' + str(len(states)) +\n",
    "                                 'state values. Input received: ' +\n",
    "                                 str(states))\n",
    "            for index, (value, state) in enumerate(zip(states, self.states)):\n",
    "                if value.shape != (batch_size, self.units):\n",
    "                    raise ValueError('State ' + str(index) +\n",
    "                                     ' is incompatible with layer ' +\n",
    "                                     self.name + ': expected shape=' +\n",
    "                                     str((batch_size, self.units)) +\n",
    "                                     ', found shape=' + str(value.shape))\n",
    "                K.set_value(state, value)\n",
    "\n",
    "    def __call__(self, inputs, initial_state=None, **kwargs):\n",
    "        # If `initial_state` is specified,\n",
    "        # and if it a Keras tensor,\n",
    "        # then add it to the inputs and temporarily\n",
    "        # modify the input spec to include the state.\n",
    "        if initial_state is not None:\n",
    "            if hasattr(initial_state, '_keras_history'):\n",
    "                # Compute the full input spec, including state\n",
    "                input_spec = self.input_spec\n",
    "                state_spec = self.state_spec\n",
    "                if not isinstance(state_spec, list):\n",
    "                    state_spec = [state_spec]\n",
    "                self.input_spec = [input_spec] + state_spec\n",
    "\n",
    "                # Compute the full inputs, including state\n",
    "                if not isinstance(initial_state, (list, tuple)):\n",
    "                    initial_state = [initial_state]\n",
    "                inputs = [inputs] + list(initial_state)\n",
    "\n",
    "                # Perform the call\n",
    "                output = super(QRNN, self).__call__(inputs, **kwargs)\n",
    "\n",
    "                # Restore original input spec\n",
    "                self.input_spec = input_spec\n",
    "                return output\n",
    "            else:\n",
    "                kwargs['initial_state'] = initial_state\n",
    "        return super(QRNN, self).__call__(inputs, **kwargs)\n",
    "\n",
    "    def call(self, inputs, mask=None, initial_state=None, training=None):\n",
    "        # input shape: `(samples, time (padded with zeros), input_dim)`\n",
    "        # note that the .build() method of subclasses MUST define\n",
    "        # self.input_spec and self.state_spec with complete input shapes.\n",
    "        if isinstance(inputs, list):\n",
    "            initial_states = inputs[1:]\n",
    "            inputs = inputs[0]\n",
    "        elif initial_state is not None:\n",
    "            pass\n",
    "        elif self.stateful:\n",
    "            initial_states = self.states\n",
    "        else:\n",
    "            initial_states = self.get_initial_states(inputs)\n",
    "\n",
    "        if len(initial_states) != len(self.states):\n",
    "            raise ValueError('Layer has ' + str(len(self.states)) +\n",
    "                             ' states but was passed ' +\n",
    "                             str(len(initial_states)) +\n",
    "                             ' initial states.')\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        if self.unroll and input_shape[1] is None:\n",
    "            raise ValueError('Cannot unroll a RNN if the '\n",
    "                             'time dimension is undefined. \\n'\n",
    "                             '- If using a Sequential model, '\n",
    "                             'specify the time dimension by passing '\n",
    "                             'an `input_shape` or `batch_input_shape` '\n",
    "                             'argument to your first layer. If your '\n",
    "                             'first layer is an Embedding, you can '\n",
    "                             'also use the `input_length` argument.\\n'\n",
    "                             '- If using the functional API, specify '\n",
    "                             'the time dimension by passing a `shape` '\n",
    "                             'or `batch_shape` argument to your Input layer.')\n",
    "        constants = self.get_constants(inputs, training=None)\n",
    "        preprocessed_input = self.preprocess_input(inputs, training=None)\n",
    "\n",
    "        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n",
    "                                            initial_states,\n",
    "                                            go_backwards=self.go_backwards,\n",
    "                                            mask=mask,\n",
    "                                            constants=constants,\n",
    "                                            unroll=self.unroll,\n",
    "                                            input_length=input_shape[1])\n",
    "        if self.stateful:\n",
    "            updates = []\n",
    "            for i in range(len(states)):\n",
    "                updates.append((self.states[i], states[i]))\n",
    "            self.add_update(updates, inputs)\n",
    "\n",
    "        # Properly set learning phase\n",
    "        if 0 < self.dropout < 1:\n",
    "            last_output._uses_learning_phase = True\n",
    "            outputs._uses_learning_phase = True\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return outputs\n",
    "        else:\n",
    "            return last_output\n",
    "\n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        if self.window_size > 1:\n",
    "            inputs = K.temporal_padding(inputs, (self.window_size-1, 0))\n",
    "        inputs = K.expand_dims(inputs, 2)  # add a dummy dimension\n",
    "\n",
    "        output = K.conv2d(inputs, self.kernel, strides=self.strides,\n",
    "                          padding='valid',\n",
    "                          data_format='channels_last')\n",
    "        output = K.squeeze(output, 2)  # remove the dummy dimension\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "\n",
    "        if self.dropout is not None and 0. < self.dropout < 1.:\n",
    "            z = output[:, :, :self.units]\n",
    "            f = output[:, :, self.units:2 * self.units]\n",
    "            o = output[:, :, 2 * self.units:]\n",
    "            f = K.in_train_phase(1 - _dropout(1 - f, self.dropout), f, training=training)\n",
    "            return K.concatenate([z, f, o], -1)\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def step(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "\n",
    "        z = inputs[:, :self.units]\n",
    "        f = inputs[:, self.units:2 * self.units]\n",
    "        o = inputs[:, 2 * self.units:]\n",
    "\n",
    "        z = self.activation(z)\n",
    "        f = f if self.dropout is not None and 0. < self.dropout < 1. else K.sigmoid(f)\n",
    "        o = K.sigmoid(o)\n",
    "\n",
    "        output = f * prev_output + (1 - f) * z\n",
    "        output = o * output\n",
    "\n",
    "        return output, [output]\n",
    "\n",
    "    def get_constants(self, inputs, training=None):\n",
    "        return []\n",
    " \n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'window_size': self.window_size,\n",
    "                  'stride': self.strides[0],\n",
    "                  'return_sequences': self.return_sequences,\n",
    "                  'go_backwards': self.go_backwards,\n",
    "                  'stateful': self.stateful,\n",
    "                  'unroll': self.unroll,\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'dropout': self.dropout,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'input_dim': self.input_dim,\n",
    "                  'input_length': self.input_length}\n",
    "        base_config = super(QRNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 120, 128)     6400384     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 120, 128)     0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 118, 256)     98560       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 116, 256)     196864      conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "qrnn_16 (QRNN)                  (None, 116, 256)     393984      conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "qrnn_17 (QRNN)                  (None, 116, 256)     393984      qrnn_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "qrnn_18 (QRNN)                  (None, 58, 512)      787968      qrnn_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "qrnn_19 (QRNN)                  (None, 512)          1574400     qrnn_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "qrnn_20 (QRNN)                  (None, 512)          1574400     qrnn_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           qrnn_19[0][0]                    \n",
      "                                                                 qrnn_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1024)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           65600       dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 64)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            130         dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 11,486,274\n",
      "Trainable params: 11,486,274\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def qrnn_model(conv_layers = 2, max_dilation_rate = 3):\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(120, ))\n",
    "    x = Embedding(50003, embed_size)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    prefilt = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    x = prefilt\n",
    "    for strides in [1, 1, 2]:\n",
    "        x = QRNN(128*2**(strides), return_sequences = True, stride = strides, dropout = 0.2, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x_f = QRNN(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n",
    "    x_b = QRNN(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10), go_backwards=True)(x)\n",
    "    x = concatenate([x_f, x_b])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(2)(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "qrnn_model = qrnn_model()\n",
    "qrnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path=\"early_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "callbacks = [checkpoint, early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 1526s 95ms/step - loss: 1.6681 - binary_accuracy: 0.4999 - val_loss: 0.7283 - val_binary_accuracy: 0.4999\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72834, saving model to early_weights.hdf5\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 1412s 88ms/step - loss: 0.7093 - binary_accuracy: 0.5397 - val_loss: 0.6385 - val_binary_accuracy: 0.5809\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72834 to 0.63850, saving model to early_weights.hdf5\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 1401s 88ms/step - loss: 0.5560 - binary_accuracy: 0.7687 - val_loss: 1.2440 - val_binary_accuracy: 0.5179\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 1402s 88ms/step - loss: nan - binary_accuracy: 0.0740 - val_loss: nan - val_binary_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/10\n",
      "15872/16000 [============================>.] - ETA: 10s - loss: nan - binary_accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "qrnn_model.fit(train_X,labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_split=0.20, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weight_sentiment_amazon.{epoch:02d}.hdf5')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = train_X[:BATCH_SIZE]\n",
    "Y_valid = labels[:BATCH_SIZE]\n",
    "train_X = train_X[BATCH_SIZE:]\n",
    "y_train_oh = labels[BATCH_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(train_X)\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "y_train_oh = np.array(y_train_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0]\n",
    "#X_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19488 samples, validate on 512 samples\n",
      "Epoch 1/26\n",
      "19488/19488 [==============================] - 73s 4ms/step - loss: 0.6698 - acc: 0.5994 - val_loss: 0.5790 - val_acc: 0.7695\n",
      "Epoch 2/26\n",
      "19488/19488 [==============================] - 70s 4ms/step - loss: 0.4731 - acc: 0.7801 - val_loss: 0.3539 - val_acc: 0.8516\n",
      "Epoch 3/26\n",
      "19488/19488 [==============================] - 69s 4ms/step - loss: 0.3384 - acc: 0.8592 - val_loss: 0.3242 - val_acc: 0.8711\n",
      "Epoch 4/26\n",
      "19488/19488 [==============================] - 70s 4ms/step - loss: 0.2799 - acc: 0.8876 - val_loss: 0.3185 - val_acc: 0.8730\n",
      "Epoch 5/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.2429 - acc: 0.9069 - val_loss: 0.3248 - val_acc: 0.8770\n",
      "Epoch 6/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.2098 - acc: 0.9209 - val_loss: 0.3362 - val_acc: 0.8848\n",
      "Epoch 7/26\n",
      "19488/19488 [==============================] - 70s 4ms/step - loss: 0.1874 - acc: 0.9293 - val_loss: 0.3527 - val_acc: 0.8828\n",
      "Epoch 8/26\n",
      "19488/19488 [==============================] - 81s 4ms/step - loss: 0.1669 - acc: 0.9392 - val_loss: 0.3792 - val_acc: 0.8828\n",
      "Epoch 9/26\n",
      "19488/19488 [==============================] - 72s 4ms/step - loss: 0.1502 - acc: 0.9478 - val_loss: 0.4598 - val_acc: 0.8574\n",
      "Epoch 10/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.1369 - acc: 0.9502 - val_loss: 0.4373 - val_acc: 0.8750\n",
      "Epoch 11/26\n",
      "19488/19488 [==============================] - 72s 4ms/step - loss: 0.1266 - acc: 0.9557 - val_loss: 0.4984 - val_acc: 0.8730\n",
      "Epoch 12/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.1124 - acc: 0.9601 - val_loss: 0.5068 - val_acc: 0.8496\n",
      "Epoch 13/26\n",
      "19488/19488 [==============================] - 76s 4ms/step - loss: 0.1099 - acc: 0.9623 - val_loss: 0.4925 - val_acc: 0.8730\n",
      "Epoch 14/26\n",
      "19488/19488 [==============================] - 73s 4ms/step - loss: 0.0894 - acc: 0.9693 - val_loss: 0.5094 - val_acc: 0.8770\n",
      "Epoch 15/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.0882 - acc: 0.9693 - val_loss: 0.5682 - val_acc: 0.8516\n",
      "Epoch 16/26\n",
      "19488/19488 [==============================] - 81s 4ms/step - loss: 0.0702 - acc: 0.9769 - val_loss: 1.3149 - val_acc: 0.7285\n",
      "Epoch 17/26\n",
      "19488/19488 [==============================] - 80s 4ms/step - loss: 0.1865 - acc: 0.9320 - val_loss: 0.5401 - val_acc: 0.8535\n",
      "Epoch 18/26\n",
      "19488/19488 [==============================] - 75s 4ms/step - loss: 0.0846 - acc: 0.9721 - val_loss: 0.5355 - val_acc: 0.8633\n",
      "Epoch 19/26\n",
      "19488/19488 [==============================] - 76s 4ms/step - loss: 0.0625 - acc: 0.9791 - val_loss: 0.5861 - val_acc: 0.8555\n",
      "Epoch 20/26\n",
      "19488/19488 [==============================] - 78s 4ms/step - loss: 0.0519 - acc: 0.9837 - val_loss: 0.6953 - val_acc: 0.8418\n",
      "Epoch 21/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.0498 - acc: 0.9842 - val_loss: 0.7302 - val_acc: 0.8340\n",
      "Epoch 22/26\n",
      "19488/19488 [==============================] - 81s 4ms/step - loss: 0.0386 - acc: 0.9872 - val_loss: 0.7357 - val_acc: 0.8438\n",
      "Epoch 23/26\n",
      "19488/19488 [==============================] - 88s 5ms/step - loss: 0.0336 - acc: 0.9889 - val_loss: 0.7560 - val_acc: 0.8418\n",
      "Epoch 24/26\n",
      "19488/19488 [==============================] - 81s 4ms/step - loss: 0.0329 - acc: 0.9893 - val_loss: 0.7454 - val_acc: 0.8457\n",
      "Epoch 25/26\n",
      "19488/19488 [==============================] - 83s 4ms/step - loss: 0.0334 - acc: 0.9889 - val_loss: 0.8488 - val_acc: 0.8457\n",
      "Epoch 26/26\n",
      "19488/19488 [==============================] - 78s 4ms/step - loss: 0.0228 - acc: 0.9926 - val_loss: 0.7499 - val_acc: 0.8438\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, y_train_oh, validation_data=(X_valid, Y_valid), callbacks=callbacks_list, epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.884765625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(inp):\n",
    "    punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "    #neg = negate_sequence(inp)\n",
    "    for i in punct:\n",
    "        inp.replace(i, '')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    inp = tokenizer.tokenize(inp)\n",
    "    MAX_SEQ = 120\n",
    "    n = MAX_SEQ - len(inp)\n",
    "    if n < 0:\n",
    "        inp = inp[:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            inp.append('<PAD>')\n",
    "    for v in range(len(inp)):\n",
    "        if inp[v] not in tk.word_index:\n",
    "            inp[v] = tk.word_index['<UNK>']\n",
    "        else:\n",
    "            inp[v] = tk.word_index[inp[v]]\n",
    "    return np.reshape(np.array(inp) , (1 , MAX_SEQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01202024,  0.98797977]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(prep_data(\"Hugely Disappointing: I've read all the previous entries in the Grantville Universe and enjoyed most of them quite a bit. But where the use of various writers and short stories worked well for a time, the quality of the writing has been steadily decreasing.The Ram Rebellion reads as a book totally abandoned by Eric Flint and handled almost entirely by the Regulars at Baen's Bar. I understand Baen's practice of pairing up-and-coming writers with more established writers in order to develop new talent and further the plot of popular series. It's worked well in the past.Here it simply failed. I'm about 1/5 through the book and am so tired of the lackluster writing, the sophomoric humor and the terribly slow plot progression that I'm putting it away for good. I'm probably unlikely to read the rest of the series because it's reached a point where having Flint's name on the cover is totally meaningless and not at all representative of the quality of writing to be expected within.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "with open(\"train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        data_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"__label__1 Hugely Disappointing: I've read all the previous entries in the Grantville Universe and enjoyed most of them quite a bit. But where the use of various writers and short stories worked well for a time, the quality of the writing has been steadily decreasing.The Ram Rebellion reads as a book totally abandoned by Eric Flint and handled almost entirely by the Regulars at Baen's Bar. I understand Baen's practice of pairing up-and-coming writers with more established writers in order to develop new talent and further the plot of popular series. It's worked well in the past.Here it simply failed. I'm about 1/5 through the book and am so tired of the lackluster writing, the sophomoric humor and the terribly slow plot progression that I'm putting it away for good. I'm probably unlikely to read the rest of the series because it's reached a point where having Flint's name on the cover is totally meaningless and not at all representative of the quality of writing to be expected within.\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataa = []\n",
    "with open(\"train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        dataa.append(i)\n",
    "        \n",
    "len(dataa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
