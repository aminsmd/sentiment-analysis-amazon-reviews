{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 50000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "NB_START_EPOCHS = 26  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data) :\n",
    "    labels = []\n",
    "    for i in data :\n",
    "        if(((i.split()[0]).replace(\"__label__\",\"\"))=='1'):\n",
    "            labels.append([1,0])\n",
    "        else :\n",
    "            labels.append([0,1])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data[0].replace(\"__label__\",\"\"))[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'!'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_label(data):\n",
    "    d = []\n",
    "    for i in data :\n",
    "        d.append((i.replace(\"__label__\",\"\"))[2:])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_label(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:20000]\n",
    "labels = labels[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_sentences(data):    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = remove_stopwords(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_sentences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \", \n",
    "               char_level=False)\n",
    "tk.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "training = []\n",
    "for i in range(len(data)):\n",
    "    training.append(tokenizer.tokenize(data[i]))\n",
    "for i in range(len(training)):\n",
    "    training[i] = [x.lower() for x in training[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.word_index['<PAD>']=NB_WORDS+1\n",
    "tk.word_index['<UNK>']=0\n",
    "tk.word_index['n\\'t'] = NB_WORDS+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "train_X =[]\n",
    "for i in range(len(training)):\n",
    "    sentence = []\n",
    "    for j in range(len(training[i])):\n",
    "        if training[i][j] in punct:\n",
    "            pass\n",
    "        else:\n",
    "            sentence.append(training[i][j])\n",
    "    train_X.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 120\n",
    "for s in range(len(train_X)):\n",
    "    n = MAX_SEQ - len(train_X[s])\n",
    "    if n < 0:\n",
    "        train_X[s] = train_X[s][:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            train_X[s].append('<PAD>')\n",
    "    for v in range(len(train_X[s])):\n",
    "        if train_X[s][v] not in tk.word_index:\n",
    "            train_X[s][v] = tk.word_index['<UNK>']\n",
    "        else:\n",
    "            train_X[s][v] = tk.word_index[train_X[s][v]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tk.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 120)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.array(train_X)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 120, 32)           1600096   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               106400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 1,706,898\n",
      "Trainable params: 1,706,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(NB_WORDS+3, embedding_size, input_length=MAX_SEQ))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.8)))\n",
    "# model.add(Dense(20, kernel_regularizer=regularizers.l1_l2(0.01)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='adam'\n",
    "              , loss='binary_crossentropy'\n",
    "              , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weight_sentiment_amazon.{epoch:02d}.hdf5')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = train_X[:BATCH_SIZE]\n",
    "Y_valid = labels[:BATCH_SIZE]\n",
    "train_X = train_X[BATCH_SIZE:]\n",
    "y_train_oh = labels[BATCH_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(train_X)\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "y_train_oh = np.array(y_train_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0]\n",
    "#X_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19488 samples, validate on 512 samples\n",
      "Epoch 1/26\n",
      "19488/19488 [==============================] - 73s 4ms/step - loss: 0.6698 - acc: 0.5994 - val_loss: 0.5790 - val_acc: 0.7695\n",
      "Epoch 2/26\n",
      "19488/19488 [==============================] - 70s 4ms/step - loss: 0.4731 - acc: 0.7801 - val_loss: 0.3539 - val_acc: 0.8516\n",
      "Epoch 3/26\n",
      "19488/19488 [==============================] - 69s 4ms/step - loss: 0.3384 - acc: 0.8592 - val_loss: 0.3242 - val_acc: 0.8711\n",
      "Epoch 4/26\n",
      "19488/19488 [==============================] - 70s 4ms/step - loss: 0.2799 - acc: 0.8876 - val_loss: 0.3185 - val_acc: 0.8730\n",
      "Epoch 5/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.2429 - acc: 0.9069 - val_loss: 0.3248 - val_acc: 0.8770\n",
      "Epoch 6/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.2098 - acc: 0.9209 - val_loss: 0.3362 - val_acc: 0.8848\n",
      "Epoch 7/26\n",
      "19488/19488 [==============================] - 70s 4ms/step - loss: 0.1874 - acc: 0.9293 - val_loss: 0.3527 - val_acc: 0.8828\n",
      "Epoch 8/26\n",
      "19488/19488 [==============================] - 81s 4ms/step - loss: 0.1669 - acc: 0.9392 - val_loss: 0.3792 - val_acc: 0.8828\n",
      "Epoch 9/26\n",
      "19488/19488 [==============================] - 72s 4ms/step - loss: 0.1502 - acc: 0.9478 - val_loss: 0.4598 - val_acc: 0.8574\n",
      "Epoch 10/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.1369 - acc: 0.9502 - val_loss: 0.4373 - val_acc: 0.8750\n",
      "Epoch 11/26\n",
      "19488/19488 [==============================] - 72s 4ms/step - loss: 0.1266 - acc: 0.9557 - val_loss: 0.4984 - val_acc: 0.8730\n",
      "Epoch 12/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.1124 - acc: 0.9601 - val_loss: 0.5068 - val_acc: 0.8496\n",
      "Epoch 13/26\n",
      "19488/19488 [==============================] - 76s 4ms/step - loss: 0.1099 - acc: 0.9623 - val_loss: 0.4925 - val_acc: 0.8730\n",
      "Epoch 14/26\n",
      "19488/19488 [==============================] - 73s 4ms/step - loss: 0.0894 - acc: 0.9693 - val_loss: 0.5094 - val_acc: 0.8770\n",
      "Epoch 15/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.0882 - acc: 0.9693 - val_loss: 0.5682 - val_acc: 0.8516\n",
      "Epoch 16/26\n",
      "19488/19488 [==============================] - 81s 4ms/step - loss: 0.0702 - acc: 0.9769 - val_loss: 1.3149 - val_acc: 0.7285\n",
      "Epoch 17/26\n",
      "19488/19488 [==============================] - 80s 4ms/step - loss: 0.1865 - acc: 0.9320 - val_loss: 0.5401 - val_acc: 0.8535\n",
      "Epoch 18/26\n",
      "19488/19488 [==============================] - 75s 4ms/step - loss: 0.0846 - acc: 0.9721 - val_loss: 0.5355 - val_acc: 0.8633\n",
      "Epoch 19/26\n",
      "19488/19488 [==============================] - 76s 4ms/step - loss: 0.0625 - acc: 0.9791 - val_loss: 0.5861 - val_acc: 0.8555\n",
      "Epoch 20/26\n",
      "19488/19488 [==============================] - 78s 4ms/step - loss: 0.0519 - acc: 0.9837 - val_loss: 0.6953 - val_acc: 0.8418\n",
      "Epoch 21/26\n",
      "19488/19488 [==============================] - 71s 4ms/step - loss: 0.0498 - acc: 0.9842 - val_loss: 0.7302 - val_acc: 0.8340\n",
      "Epoch 22/26\n",
      "19488/19488 [==============================] - 81s 4ms/step - loss: 0.0386 - acc: 0.9872 - val_loss: 0.7357 - val_acc: 0.8438\n",
      "Epoch 23/26\n",
      "19488/19488 [==============================] - 88s 5ms/step - loss: 0.0336 - acc: 0.9889 - val_loss: 0.7560 - val_acc: 0.8418\n",
      "Epoch 24/26\n",
      "19488/19488 [==============================] - 81s 4ms/step - loss: 0.0329 - acc: 0.9893 - val_loss: 0.7454 - val_acc: 0.8457\n",
      "Epoch 25/26\n",
      "19488/19488 [==============================] - 83s 4ms/step - loss: 0.0334 - acc: 0.9889 - val_loss: 0.8488 - val_acc: 0.8457\n",
      "Epoch 26/26\n",
      "19488/19488 [==============================] - 78s 4ms/step - loss: 0.0228 - acc: 0.9926 - val_loss: 0.7499 - val_acc: 0.8438\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, y_train_oh, validation_data=(X_valid, Y_valid), callbacks=callbacks_list, epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.884765625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(inp):\n",
    "    punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "    #neg = negate_sequence(inp)\n",
    "    for i in punct:\n",
    "        inp.replace(i, '')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    inp = tokenizer.tokenize(inp)\n",
    "    MAX_SEQ = 120\n",
    "    n = MAX_SEQ - len(inp)\n",
    "    if n < 0:\n",
    "        inp = inp[:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            inp.append('<PAD>')\n",
    "    for v in range(len(inp)):\n",
    "        if inp[v] not in tk.word_index:\n",
    "            inp[v] = tk.word_index['<UNK>']\n",
    "        else:\n",
    "            inp[v] = tk.word_index[inp[v]]\n",
    "    return np.reshape(np.array(inp) , (1 , MAX_SEQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01202024,  0.98797977]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(prep_data(\"Hugely Disappointing: I've read all the previous entries in the Grantville Universe and enjoyed most of them quite a bit. But where the use of various writers and short stories worked well for a time, the quality of the writing has been steadily decreasing.The Ram Rebellion reads as a book totally abandoned by Eric Flint and handled almost entirely by the Regulars at Baen's Bar. I understand Baen's practice of pairing up-and-coming writers with more established writers in order to develop new talent and further the plot of popular series. It's worked well in the past.Here it simply failed. I'm about 1/5 through the book and am so tired of the lackluster writing, the sophomoric humor and the terribly slow plot progression that I'm putting it away for good. I'm probably unlikely to read the rest of the series because it's reached a point where having Flint's name on the cover is totally meaningless and not at all representative of the quality of writing to be expected within.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "with open(\"train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        data_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"__label__1 Hugely Disappointing: I've read all the previous entries in the Grantville Universe and enjoyed most of them quite a bit. But where the use of various writers and short stories worked well for a time, the quality of the writing has been steadily decreasing.The Ram Rebellion reads as a book totally abandoned by Eric Flint and handled almost entirely by the Regulars at Baen's Bar. I understand Baen's practice of pairing up-and-coming writers with more established writers in order to develop new talent and further the plot of popular series. It's worked well in the past.Here it simply failed. I'm about 1/5 through the book and am so tired of the lackluster writing, the sophomoric humor and the terribly slow plot progression that I'm putting it away for good. I'm probably unlikely to read the rest of the series because it's reached a point where having Flint's name on the cover is totally meaningless and not at all representative of the quality of writing to be expected within.\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataa = []\n",
    "with open(\"train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        dataa.append(i)\n",
    "        \n",
    "len(dataa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
