{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import pickle as pk\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shamim imports\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import sys\n",
    "import codecs\n",
    "from nltk.classify import NaiveBayesClassifier, DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.sentiment import SentimentAnalyzer\n",
    "#import string \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"../train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing data and extracting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TreebankWordTokenizer()\n",
    "labels = []\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = tk.tokenize(train_data[i])\n",
    "    labels.append(train_data[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    labels[i] = labels[i].replace(\"__label__\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(words): #remove punctuation and convert to lower letters\n",
    "    return [word for word in words if word.isalpha()]\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    return [w for w in words if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i] = remove_punctuation(train_data[i])\n",
    "    train_data[i] = remove_stop_words(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Excellent',\n",
       " 'Soundtrack',\n",
       " 'truly',\n",
       " 'like',\n",
       " 'soundtrack',\n",
       " 'enjoy',\n",
       " 'video',\n",
       " 'game',\n",
       " 'played',\n",
       " 'game',\n",
       " 'music',\n",
       " 'enjoy',\n",
       " 'truly',\n",
       " 'relaxing',\n",
       " 'disk',\n",
       " 'favorites',\n",
       " 'Scars',\n",
       " 'Time',\n",
       " 'Life',\n",
       " 'Death',\n",
       " 'Forest',\n",
       " 'Illusion',\n",
       " 'Fortress',\n",
       " 'Ancient',\n",
       " 'Dragons',\n",
       " 'Lost',\n",
       " 'Fragment',\n",
       " 'Drowned',\n",
       " 'Two',\n",
       " 'Draggons',\n",
       " 'Galdorb',\n",
       " 'Home',\n",
       " 'Chronomantique',\n",
       " 'Prisoners',\n",
       " 'Fate',\n",
       " 'Gale',\n",
       " 'girlfriend',\n",
       " 'likes',\n",
       " 'ZelbessDisk',\n",
       " 'Three',\n",
       " 'best',\n",
       " 'Garden',\n",
       " 'God',\n",
       " 'Chronopolis',\n",
       " 'Fates',\n",
       " 'Jellyfish',\n",
       " 'sea',\n",
       " 'Burning',\n",
       " 'Orphange',\n",
       " 'Dragon',\n",
       " 'Prayer',\n",
       " 'Tower',\n",
       " 'Stars',\n",
       " 'Dragon',\n",
       " 'God',\n",
       " 'Radical',\n",
       " 'Dreamers',\n",
       " 'Unstealable',\n",
       " 'excellent',\n",
       " 'soundtrack',\n",
       " 'brought',\n",
       " 'like',\n",
       " 'video',\n",
       " 'game',\n",
       " 'Cross']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvp = CountVectorizer()\n",
    "cntvn = CountVectorizer()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_str(content,labels):\n",
    "    ones = \"\"\n",
    "    twos = \"\"\n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            if labels[i] == '1':\n",
    "                ones += content[i][j] + ' '\n",
    "            else :\n",
    "                twos += content[i][j] + ' '\n",
    "    return ones,twos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-17707f3fab74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mones\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtwos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcntvp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salam , khoobi?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcntvn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \"\"\"\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             raise ValueError(\n\u001b[0;32m--> 860\u001b[0;31m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m                 \"string object received.\")\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "ones , twos = words_to_str(train_data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
